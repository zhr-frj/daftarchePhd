{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhr-frj/daftarchePhd/blob/main/Copy_of_Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W0CPCljcr5F",
        "outputId": "6ec58b98-7da6-4614-ed0b-0a902085c855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 14 06:16:05 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPWK7nYxcGPx",
        "outputId": "d372027b-7968-4cc4-b7db-f15488f64445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ ---\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m326.6/326.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m--- Ù†ØµØ¨ Ú©Ø§Ù…Ù„ Ø´Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- Ø³Ù„ÙˆÙ„ Û±: Ù†ØµØ¨ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ùˆ Ù¾Ø§ÛŒØ¯Ø§Ø± ---\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ ---\")\n",
        "!pip install langchain langchain-community transformers bitsandbytes accelerate faiss-cpu pypdf sentence-transformers langchain_huggingface docarray -q\n",
        "print(\"--- Ù†ØµØ¨ Ú©Ø§Ù…Ù„ Ø´Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iUDnauFYxFq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d6873e-602e-4136-c801-98438796f14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ (Quantization) ---\n",
            "--- Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ---\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from transformers import BitsAndBytesConfig\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ (Quantization) ---\")\n",
        "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ 4-bit Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "print(\"--- Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZrAJvITztf9",
        "outputId": "9d1e6dcd-9103-4a16-ada5-34dca020ed5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØªØ±Ø¬Ù…Â» (Gemma 2B) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efc75defd7464732a6e5a7c2ba5c8fb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4690cad53d9c4187a2ec2ddb6c6d21e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "839aa8fe784a4916aa02d4824b61a8ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de303c332aa148d3a71689113edd76e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Â«Ù…ØªØ±Ø¬Ù…Â» (Tokenizer) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# # --- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØªØ±Ø¬Ù…Â» (Tokenizer) ---\n",
        "# print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØªØ±Ø¬Ù…Â» (Tokenizer) ---\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"unsloth/gemma-2-9b-it\", use_fast=True)\n",
        "# print(\"--- Â«Ù…ØªØ±Ø¬Ù…Â» (Tokenizer) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\")\n",
        "\n",
        "\n",
        "# --- Ø³Ù„ÙˆÙ„ Û².Û²: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØªØ±Ø¬Ù…Â» (Tokenizer) - Ù†Ø³Ø®Ù‡ Ø³Ø¨Ú© Gemma 2B ---\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØªØ±Ø¬Ù…Â» (Gemma 2B) ---\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"unsloth/gemma-2-2b-it\",\n",
        "    use_fast=True\n",
        "    )\n",
        "print(\"--- Â«Ù…ØªØ±Ø¬Ù…Â» (Tokenizer) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TihILDUIzx7o",
        "outputId": "c641791a-7352-4539-d37b-0bcce12e4ed7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØºØ²Â» (LLM) --- (Ù†Ø³Ø®Ù‡ Ø³Ø¨Ú© Gemma 2B)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/913 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36f99456fc5e415793fe1f766113737f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.23G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6374b941a3334240908d650233bf162d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/209 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0bf42562dde943baa9e1cb5c5699f52b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Â«Ù…ØºØ²Â» (LLM) Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# # --- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØºØ²Â» (LLM) ---\n",
        "# print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØºØ²Â» (LLM) --- (Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ø¨Ø³ÛŒØ§Ø± Ø³Ù†Ú¯ÛŒÙ† Ùˆ Ø²Ù…Ø§Ù†â€ŒØ¨Ø± Ø§Ø³Øª)\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"unsloth/gemma-2-9b-it\",\n",
        "#     device_map=\"auto\",\n",
        "#     quantization_config=bnb_config,\n",
        "#     torch_dtype=torch.float16\n",
        "# )\n",
        "# print(\"--- Â«Ù…ØºØ²Â» (LLM) Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Ø³Ù„ÙˆÙ„ Û².Û³: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØºØ²Â» (LLM) - Ù†Ø³Ø®Ù‡ Ø³Ø¨Ú© Gemma 2B ---\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØºØ²Â» (LLM) --- (Ù†Ø³Ø®Ù‡ Ø³Ø¨Ú© Gemma 2B)\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"unsloth/gemma-2-2b-it\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config, # Ø§Ø² bnb_config Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "print(\"--- Â«Ù…ØºØ²Â» (LLM) Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aXgzl26z5Br",
        "outputId": "bad0db42-13f1-49ad-d01e-089fe8588d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª Â«Ø®Ø· Ù„ÙˆÙ„Ù‡Â» (Pipeline) ---\n",
            "--- Â«Ø®Ø· Ù„ÙˆÙ„Ù‡Â»ØŒ Â«ØºÙ„Ø§ÙÂ» Ùˆ Â«ØªÙ…ÛŒØ²Ú©Ù†Ù†Ø¯Ù‡Â» Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- Ø³Ø§Ø®Øª Â«Ø®Ø· Ù„ÙˆÙ„Ù‡Â» Ùˆ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ LangChain ---\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª Â«Ø®Ø· Ù„ÙˆÙ„Ù‡Â» (Pipeline) ---\")\n",
        "text_gen_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512  # Ø§ÙØ²Ø§ÛŒØ´ Ø·ÙˆÙ„ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ø¬ÙˆØ§Ø¨â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„â€ŒØªØ±\n",
        ")\n",
        "\n",
        "# Ø³Ø§Ø®Øª Â«ØºÙ„Ø§ÙÂ» LangChain Ø¨Ø±Ø§ÛŒ LLM\n",
        "llm = HuggingFacePipeline(pipeline=text_gen_pipeline)\n",
        "\n",
        "# Ø³Ø§Ø®Øª Â«ØªÙ…ÛŒØ²Ú©Ù†Ù†Ø¯Ù‡ Ø®Ø±ÙˆØ¬ÛŒÂ»\n",
        "parser = StrOutputParser()\n",
        "\n",
        "print(\"--- Â«Ø®Ø· Ù„ÙˆÙ„Ù‡Â»ØŒ Â«ØºÙ„Ø§ÙÂ» Ùˆ Â«ØªÙ…ÛŒØ²Ú©Ù†Ù†Ø¯Ù‡Â» Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6wHqoJq0TiM",
        "outputId": "0d8ef3cf-a719-48e2-8763-b164d3c9d554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Â«Ù‚Ø§Ù„Ø¨ Ú†ØªÂ» Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¹Ù…Ø§Ù„ Â«Ù‚Ø§Ù„Ø¨ Ú†ØªÂ» ---\n",
        "\n",
        "# Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ØŒ Ù¾Ø±Ø§Ù…Ù¾Øª Ù†Ù‡Ø§ÛŒÛŒ Ù…Ø§ Ø±Ø§ Ø¯Ø± Ù‚Ø§Ù„Ø¨ Ú†Øª Ø±Ø³Ù…ÛŒ Ù…Ø¯Ù„ Gemma Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡Ø¯\n",
        "def apply_chat_template_and_response(prompt):\n",
        "    messages = [\n",
        "    {'role': 'user', 'content': prompt}\n",
        "    ]\n",
        "\n",
        "    # Ø§Ø¹Ù…Ø§Ù„ Ù‚Ø§Ù„Ø¨ Ú†Øª\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False\n",
        "    )\n",
        "\n",
        "    # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…Ø¯Ù„ Ùˆ ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ Ø§ÙˆÙ„ÛŒÙ‡\n",
        "    response = llm.invoke(text)\n",
        "    # response Ø±Ø§ ØªÙ…ÛŒØ² Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ ÙÙ‚Ø· Ø¬ÙˆØ§Ø¨ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§Ù‚ÛŒ Ø¨Ù…Ø§Ù†Ø¯\n",
        "    return response.replace(text, '').strip()\n",
        "\n",
        "print(\"--- ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Â«Ù‚Ø§Ù„Ø¨ Ú†ØªÂ» Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-oLghHF0WSw",
        "outputId": "b1449126-fe5c-491f-ef60-07765a9d34c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ø¨ÛŒÙ†ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒÙÙ‡Ù…Â» (Embeddings) ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87d2aa2e66344f058856fea2319d2ed9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "981d4eab801a445c82fcf9df220d8f1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ad294d4599d46d0b71b69e3551b7d20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2dfcbcce9a7a41578481816799461aa6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42a3a579c06b459f85d2e06eb5393ad4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "213dc7295b4f429f8083a081cae43cfa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6cd9ad6bbfc485f844ec58605e4e486"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc90ec51083947dbaf616c6740eeb2aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "096bb971da8c462987ae33ffc1605583"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "484d352842c74d60850c92671a77a41a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5bd4ac1f3f74e69aa9c6a72a44125a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Â«Ø¨ÛŒÙ†ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒÙÙ‡Ù…Â» (Embeddings) Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- Ø³Ù„ÙˆÙ„ Û´: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ø¨ÛŒÙ†ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒÙÙ‡Ù…Â» (Embedding Model) ---\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ø¨ÛŒÙ†ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒÙÙ‡Ù…Â» (Embeddings) ---\")\n",
        "\n",
        "# --- ØªØºÛŒÛŒØ± Ú©Ù„ÛŒØ¯ÛŒ ---\n",
        "# Ø¨Ù‡ Ø¬Ø§ÛŒ 'sbunlp/fabert' Ø§Ø² Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù‚ÙˆÛŒâ€ŒØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n",
        "embedding_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
        "# --- Ù¾Ø§ÛŒØ§Ù† ØªØºÛŒÛŒØ± ---\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_name,\n",
        "    model_kwargs={'device': 'cpu'} # Ù‡Ù…Ú†Ù†Ø§Ù† Ø±ÙˆÛŒ CPU Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ GPU Ø¢Ø²Ø§Ø¯ Ø¨Ù…Ø§Ù†Ø¯\n",
        ")\n",
        "print(\"--- Â«Ø¨ÛŒÙ†ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒÙÙ‡Ù…Â» (Embeddings) Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMM1J-0N0hCQ",
        "outputId": "0783d174-9177-44e8-b9e9-2b23a58b1f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Â«Ø¨Ø±Ú¯Ù‡ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„Â» (Prompt) Ù…Ø®ØµÙˆØµ Gemma Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# # --- ØªØ¹Ø±ÛŒÙ Â«Ø¨Ø±Ú¯Ù‡ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„Â» (Prompt Template) ---\n",
        "\n",
        "# from langchain.prompts import PromptTemplate\n",
        "\n",
        "# # Ø§ÛŒÙ† Ø§Ù„Ú¯Ùˆ Ø¨Ù‡ Ø±Ø¨Ø§Øª Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯ Ú©Ù‡ Â«Ø®Ø§Ø±Ø¬ Ø§Ø² PDFÂ» Ø¬ÙˆØ§Ø¨ Ù†Ø¯Ù‡Ø¯\n",
        "# template = \"\"\"\n",
        "# Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ø¨Ø³ÛŒØ§Ø± Ø¯Ù‚ÛŒÙ‚ Ù‡Ø³ØªÛŒØ¯. ÙˆØ¸ÛŒÙÙ‡ Ø´Ù…Ø§ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙ†ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø¨Ø®Ø´ Â«Ø²Ù…ÛŒÙ†Ù‡Â» Ø¨Ù‡ Ø´Ù…Ø§ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "# Ø¨Ù‡ Ù‡ÛŒÚ† ÙˆØ¬Ù‡ Ø§Ø² Ø¯Ø§Ù†Ø´ Ù‚Ø¨Ù„ÛŒ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯.\n",
        "# Ù¾Ø§Ø³Ø® Ø±Ø§ ÙÙ‚Ø· Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.\n",
        "# Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø³ÙˆØ§Ù„ Ø¯Ø± Â«Ø²Ù…ÛŒÙ†Ù‡Â» ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªØŒ Ø¨Ù‡ Ø³Ø§Ø¯Ú¯ÛŒ Ø¨Ú¯ÙˆÛŒÛŒØ¯: \"Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù….\"\n",
        "\n",
        "# Ø²Ù…ÛŒÙ†Ù‡ (Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡ Ø§Ø² PDF):\n",
        "# {context}\n",
        "\n",
        "# Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±:\n",
        "# {question}\n",
        "\n",
        "# Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ø´Ù…Ø§:\n",
        "# \"\"\"\n",
        "\n",
        "# prompt = PromptTemplate.from_template(template)\n",
        "# print(\"--- Â«Ø¨Ø±Ú¯Ù‡ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„Â» (Prompt) Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ---\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Ø³Ù„ÙˆÙ„ Ûµ: ØªØ¹Ø±ÛŒÙ Â«Ø¨Ø±Ú¯Ù‡ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„Â» (Prompt Template) - Ù†Ø³Ø®Ù‡ Gemma ---\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Ù‚Ø§Ù„Ø¨ Ù¾Ø±Ø§Ù…Ù¾Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ (Ø¨Ø¯ÙˆÙ† ØªÚ¯â€ŒÙ‡Ø§ÛŒ <|user|>)\n",
        "template = \"\"\"\n",
        "Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ø¨Ø³ÛŒØ§Ø± Ø¯Ù‚ÛŒÙ‚ Ù‡Ø³ØªÛŒØ¯. ÙˆØ¸ÛŒÙÙ‡ Ø´Ù…Ø§ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙ†ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø¨Ø®Ø´ Â«Ø²Ù…ÛŒÙ†Ù‡Â» Ø¨Ù‡ Ø´Ù…Ø§ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "Ø¨Ù‡ Ù‡ÛŒÚ† ÙˆØ¬Ù‡ Ø§Ø² Ø¯Ø§Ù†Ø´ Ù‚Ø¨Ù„ÛŒ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯.\n",
        "Ù¾Ø§Ø³Ø® Ø±Ø§ ÙÙ‚Ø· Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.\n",
        "Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø³ÙˆØ§Ù„ Ø¯Ø± Â«Ø²Ù…ÛŒÙ†Ù‡Â» ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªØŒ Ø¨Ù‡ Ø³Ø§Ø¯Ú¯ÛŒ Ø¨Ú¯ÙˆÛŒÛŒØ¯: \"Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù….\"\n",
        "\n",
        "Ø²Ù…ÛŒÙ†Ù‡ (Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡ Ø§Ø² PDF):\n",
        "{context}\n",
        "\n",
        "Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±:\n",
        "{question}\n",
        "\n",
        "Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ø´Ù…Ø§:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "print(\"--- Â«Ø¨Ø±Ú¯Ù‡ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„Â» (Prompt) Ù…Ø®ØµÙˆØµ Gemma Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWOmShtw0iBv",
        "outputId": "6f797f0e-7523-4972-9487-06c1c3bd35c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ PDF ---\n",
            "--- ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ PDF ---\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ PDF ---\")\n",
        "!wget -q \"https://github.com/zhr-frj/daftarchePhd/releases/download/v1.0.0/Phd1405-.konkur.in.pdf\" -O daftarche_asli.pdf\n",
        "!wget -q \"https://github.com/zhr-frj/daftarchePhd/releases/download/v1.0.0/Eslahiye-PHD1405-.konkur.in.pdf\" -O daftarche_eslahiye.pdf\n",
        "print(\"--- ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97d3gHt71QfS",
        "outputId": "a4fcb60d-15a5-4165-e2d2-28868946cfec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ PDF\n",
            "--- Ù…Ø¬Ù…ÙˆØ¹Ø§ 58 ØµÙØ­Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\n",
            "... Ø¯Ø± Ø­Ø§Ù„ Ø®ÙØ±Ø¯ Ú©Ø±Ø¯Ù† Ù…ØªÙ†â€ŒÙ‡Ø§\n",
            "--- Ù…ØªÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ 206 ØªÚ©Ù‡ (Chunk) ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯Ù†Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- Ø®ÙˆØ§Ù†Ø¯Ù†ØŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø®ÙØ±Ø¯ Ú©Ø±Ø¯Ù† Ù…ØªÙ†â€ŒÙ‡Ø§ ---\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Û±. Ø®ÙˆØ§Ù†Ø¯Ù† Ù‡Ø± Ø¯Ùˆ ÙØ§ÛŒÙ„ PDF\n",
        "print(\"... Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ PDF\")\n",
        "loader_asli = PyPDFLoader(\"daftarche_asli.pdf\")\n",
        "loader_eslahiye = PyPDFLoader(\"daftarche_eslahiye.pdf\")\n",
        "all_pages = loader_asli.load() + loader_eslahiye.load()\n",
        "print(f\"--- Ù…Ø¬Ù…ÙˆØ¹Ø§ {len(all_pages)} ØµÙØ­Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\")\n",
        "\n",
        "# Û². Ø®ÙØ±Ø¯ Ú©Ø±Ø¯Ù† Ù…ØªÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ ØªÚ©Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú© (Chunks)\n",
        "print(\"... Ø¯Ø± Ø­Ø§Ù„ Ø®ÙØ±Ø¯ Ú©Ø±Ø¯Ù† Ù…ØªÙ†â€ŒÙ‡Ø§\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)\n",
        "text_documents = text_splitter.split_documents(all_pages)\n",
        "print(f\"--- Ù…ØªÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ {len(text_documents)} ØªÚ©Ù‡ (Chunk) ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯Ù†Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKEaDg631UKF",
        "outputId": "d34a7b8e-994c-4bec-da28-923aca980e69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» (Vectorstore) ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª...\n",
            "--- Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ùˆ Ø¯Ø± faiss_index_daftarche Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- Ø³Ø§Ø®Øª Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» Ùˆ Â«Ú©ØªØ§Ø¨Ø¯Ø§Ø±Â» ---\n",
        "# from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "# # Û±. Ø³Ø§Ø®Øª Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» (Vectorstore)\n",
        "# # (Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ ØªÙ…Ø§Ù… ØªÚ©Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ÙˆÚ©ØªÙˆØ± ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø³Ù†Ú¯ÛŒÙ†â€ŒØªØ±ÛŒÙ† Ø¨Ø®Ø´ Ø§Ø³Øª)\n",
        "# print(\"... Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» (Vectorstore)... (Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ø²Ù…Ø§Ù†â€ŒØ¨Ø± Ø§Ø³Øª)\")\n",
        "# vectorstore = DocArrayInMemorySearch.from_documents(\n",
        "#     text_documents,\n",
        "#     embedding=embeddings # Ø§Ø² Ù…ØªØºÛŒØ± embeddings Ú©Ù‡ Ø¯Ø± Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø³Ø§Ø®ØªÛŒÙ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
        "# )\n",
        "# print(\"--- Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯ ---\")\n",
        "\n",
        "# # Û². Ø³Ø§Ø®Øª Â«Ú©ØªØ§Ø¨Ø¯Ø§Ø±Â» (Retriever)\n",
        "# retriever = vectorstore.as_retriever(search_kwargs={'k': 3}) # Û³ Ø³Ù†Ø¯ Ù…Ø±ØªØ¨Ø· Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†\n",
        "# print(\"--- Â«Ú©ØªØ§Ø¨Ø¯Ø§Ø±Â» (Retriever) Ø¢Ù…Ø§Ø¯Ù‡ Ø¬Ø³ØªØ¬Ùˆ Ø§Ø³Øª ---\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Ø³Ù„ÙˆÙ„ Û¶.Û³ (Ø§Ù„Ù): Ø³Ø§Ø®Øª ÛŒØ§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» ---\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import os\n",
        "\n",
        "DB_FAISS_PATH = \"faiss_index_daftarche\" # Ù†Ø§Ù… Ù¾ÙˆØ´Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ø¯Ø± Ø¢Ù† Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
        "\n",
        "# Ø­Ø°Ù Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ (Ú†ÙˆÙ† Ù…Ø¯Ù„ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ Ø¹ÙˆØ¶ Ø´Ø¯Ù‡)\n",
        "# !rm -rf faiss_index_daftarche\n",
        "\n",
        "if not os.path.exists(DB_FAISS_PATH):\n",
        "    print(\"... Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» (Vectorstore) ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª...\")\n",
        "    # (Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ ØªÙ…Ø§Ù… ØªÚ©Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ÙˆÚ©ØªÙˆØ± ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø³Ù†Ú¯ÛŒÙ†â€ŒØªØ±ÛŒÙ† Ø¨Ø®Ø´ Ø§Ø³Øª)\n",
        "    vectorstore = FAISS.from_documents(\n",
        "        text_documents,\n",
        "        embedding=embeddings # Ø§Ø² Ù…ØªØºÛŒØ± embeddings Ù‚Ø¨Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
        "    )\n",
        "\n",
        "    # --- Ø°Ø®ÛŒØ±Ù‡ Ú©Ø±Ø¯Ù† Â«Ù…ØºØ²Â» Ø±ÙˆÛŒ Ø¯ÛŒØ³Ú© ---\n",
        "    vectorstore.save_local(DB_FAISS_PATH)\n",
        "    print(f\"--- Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ùˆ Ø¯Ø± {DB_FAISS_PATH} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ ---\")\n",
        "\n",
        "else:\n",
        "    print(f\"--- Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» Ø§Ø² Ù‚Ø¨Ù„ Ø¯Ø± {DB_FAISS_PATH} ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯. Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ...\")\n",
        "    # --- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØºØ²Â» Ø§Ø² Ø¯ÛŒØ³Ú© (Ø¨Ø³ÛŒØ§Ø± Ø³Ø±ÛŒØ¹) ---\n",
        "    vectorstore = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "    print(\"--- Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Ø³Ù„ÙˆÙ„ Û¶.Û³ (Ø¨): Ø³Ø§Ø®Øª Â«Ú©ØªØ§Ø¨Ø¯Ø§Ø±Â» (Retriever) ---\n",
        "# (Ø§ÛŒÙ† Ø³Ù„ÙˆÙ„ Ø¨Ø§ÛŒØ¯ Ø¨Ø¹Ø¯ Ø§Ø² Ø³Ù„ÙˆÙ„ Û¶.Û³ (Ø§Ù„Ù) Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={'k': 3}) # Û³ Ø³Ù†Ø¯ Ù…Ø±ØªØ¨Ø· Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†\n",
        "print(\"--- Â«Ú©ØªØ§Ø¨Ø¯Ø§Ø±Â» (Retriever) Ø¢Ù…Ø§Ø¯Ù‡ Ø¬Ø³ØªØ¬Ùˆ Ø§Ø³Øª ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWAdy8JlFodx",
        "outputId": "ffcef293-bed9-4c6f-a2dd-ec4f496ddc1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Â«Ú©ØªØ§Ø¨Ø¯Ø§Ø±Â» (Retriever) Ø¢Ù…Ø§Ø¯Ù‡ Ø¬Ø³ØªØ¬Ùˆ Ø§Ø³Øª ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGmdoWAC1Yvr",
        "outputId": "67ca4318-e002-40b9-cec7-9a183c149e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ 'ask_robot' ---\n",
            "--- ØªØ§Ø¨Ø¹ 'ask_robot' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª ---\n"
          ]
        }
      ],
      "source": [
        "# --- ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ Ù†Ù‡Ø§ÛŒÛŒ Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø® ---\n",
        "\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ 'ask_robot' ---\")\n",
        "\n",
        "def ask_robot(question):\n",
        "    \"\"\"\n",
        "    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ÛŒÚ© Ø³ÙˆØ§Ù„ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ùˆ ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ RAG Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\")\n",
        "\n",
        "        # Û±. Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÚ©Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ† Ù…Ø±ØªØ¨Ø· (Retrieve)\n",
        "        retrieved_context_docs = retriever.invoke(question)\n",
        "        # ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ³Øª Ø§Ø³Ù†Ø§Ø¯ Ø¨Ù‡ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø­Ø¯\n",
        "        retrieved_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_context_docs)\n",
        "\n",
        "        print(\"... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\")\n",
        "\n",
        "        # Û². Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Â«Ø²Ù…ÛŒÙ†Ù‡Â» Ùˆ Â«Ø³ÙˆØ§Ù„Â» (Augment)\n",
        "        formatted_prompt = prompt.format(context=retrieved_context, question=question)\n",
        "\n",
        "        # Û³. Ø§Ø±Ø³Ø§Ù„ Ù¾Ø±Ø§Ù…Ù¾Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ù‡ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¬ÙˆØ§Ø¨ (Generate)\n",
        "        response_from_model = apply_chat_template_and_response(formatted_prompt)\n",
        "\n",
        "        # Û´. ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ø¬ÙˆØ§Ø¨\n",
        "        parsed_response = parser.parse(response_from_model)\n",
        "        return parsed_response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø±Ø® Ø¯Ø§Ø¯: {e}\"\n",
        "\n",
        "print(\"--- ØªØ§Ø¨Ø¹ 'ask_robot' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0c8g3f02QO3",
        "outputId": "24ceedc2-a4db-48a1-e3f4-b0a6778399f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ğŸ¤– Ø´Ø±ÙˆØ¹ ØªØ³Øª Ø±Ø¨Ø§Øª Ø¨Ø§ Ø³ÙˆØ§Ù„Ø§Øª Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ ğŸ¤– ---\n",
            "[Ø´Ù…Ø§]: Ø³Ù‡Ù…ÛŒÙ‡ Ø§ÛŒØ«Ø§Ø±Ú¯Ø±Ø§Ù† Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ\n",
            "... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\n",
            "... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\n",
            "\n",
            "[Ø±Ø¨Ø§Øª]:\n",
            "Ø³Ù‡Ù…ÛŒÙ‡ Ø§ÛŒØ«Ø§Ø±Ú¯Ø±Ø§Ù†ØŒ  Ø§Ø²  ÛµÛ² Ø¯Ø±ØµØ¯ ØªØ§ Ûµ Ø¯Ø±ØµØ¯ Ø§Ø² Ø¸Ø±ÙÛŒØª Ù‡Ø± Ú©Ø¯Ø±Ø´ØªÙ‡Ù…Ø­Ù„ Ø¨Ù‡  Â«Ù‡Ù…Ø³Ø± Ùˆ ÙØ±Ø²Ù†Ø¯Ø§Ù†Ø´Ù‡Ø¯Ø§ Ùˆ Ù…ÙÙ‚ÙˆØ¯Ø§Ù„Ø§Ø«Ø±Ø§Ù†Â»ØŒÂ«Ø¢Ø²Ø§Ø¯Ú¯Ø§Ù† Ùˆ Ù‡Ù…Ø³Ø± Ùˆ ÙØ±Ø²Ù†Ø¯Ø§Ù† Ø¢Ù†Ø§Ù†Â»ØŒÂ«Ø¬Ø§Ù†Ø¨Ø§Ø²Ø§Ù†ÛµÛ²% Ùˆ Ø¨Ø§Ù„Ø§ØªØ± Ùˆ Ù‡Ù…Ø³Ø± Ùˆ ÙØ±Ø²Ù†Ø¯Ø§Ù† Ø¢Ù†Ø§Ù†Â»  Ø§Ø®ØªØµØ§Øµ Ø¯Ø§Ø±Ø¯.\n",
            "\n",
            "==================================================\n",
            "[Ø´Ù…Ø§]: Ú¯Ø±Ø§ÛŒØ´ Ù‡Ø§ÛŒ Ø±Ø´ØªÙ‡ Ø²Ø¨Ø§Ù† Ùˆ Ø§Ø¯Ø¨ÛŒØ§Øª ÙØ§Ø±Ø³ÛŒØŸ\n",
            "... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\n",
            "... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Ø±Ø¨Ø§Øª]:\n",
            "Ø±Ø´ØªÙ‡ Ú¯Ø±Ø§ÙŠØ´ \n",
            "Ú©Ø¯ \n",
            "ØªØ±Ú©ÙŠØ¨  \n",
            "ØªØ±Ú©ÙŠØ¨ Ø¯Ø±ÙˆØ³ Ø§Ù…ØªØ­Ø§Ù†ÙŠ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø±ÙˆØ³ \n",
            "1 2 3  \n",
            " Ø¢Ù…ÙˆØ²Ø´ Ø²Ø¨Ø§Ù† Ùˆ Ø§Ø¯Ø¨ÙŠØ§Øª ÙØ§Ø±Ø³ÛŒ - 1 ï ï \n",
            "Ø±Ø´ØªÙ‡Ù‡Ø§ÙŠ Ù…Ø±ØªØ¨Ø· Ú©Ù‡ ÙØ§Ø±ØºØ§Ù„ØªØ­ØµÙŠÙ„Ø§Ù† Ø¢Ù† Ù…ÙŠØªÙˆØ§Ù†Ù†Ø¯ Ø¯Ø± Ø§ÙŠÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§Ù…ØªØ­Ø§Ù†ÙŠ Ø´Ø±Ú©Øª Ú©Ù†Ù†Ø¯: \n",
            "Ø²Ø¨Ø§Ù† Ùˆ Ø§Ø¯Ø¨ÙŠØ§Øª ÙØ§Ø±Ø³ÛŒ\n",
            "\n",
            "==================================================\n",
            "[Ø´Ù…Ø§]: Ú¯Ø±Ø§ÛŒØ´ Ù‡Ø§ÛŒ Ø¹Ù„ÙˆÙ… Ø§Ù‚ØªØµØ§Ø¯ÙŠØŸ\n",
            "... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\n",
            "... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\n",
            "\n",
            "[Ø±Ø¨Ø§Øª]:\n",
            "1) Ø§Ù‚ØªØµØ§Ø¯Ø³Ù†Ø¬ÙŠ \n",
            "2) Ø§Ù‚ØªØµØ§Ø¯ Ù¾ÙˆÙ„ÙŠ \n",
            "3) Ø§Ù‚ØªØµØ§Ø¯ Ø§Ø³Ù„Ø§Ù…ÙŠ \n",
            "4) Ø§Ù‚ØªØµØ§Ø¯ Ø¨Ø®Ø´ Ø¹Ù…ÙˆÙ…ÙŠ \n",
            "5) ØªÙˆØ³Ø¹Ù‡ Ø§Ù‚ØªØµØ§Ø¯ÙŠ \n",
            "6) Ø§Ù‚ØªØµØ§Ø¯ Ø¨ÛŒÙ†Ø§Ù„Ù…Ù„Ù„ \n",
            "7) Ø§Ù‚ØªØµØ§Ø¯ Ù…Ø§Ù„ÙŠ \n",
            "8) Ø§Ù‚ØªØµØ§Ø¯ Ø´Ù‡Ø±ÙŠ Ùˆ Ù…Ù†Ø·Ù‚Ù‡Ø§ÙŠ \n",
            "9) Ø§Ù‚ØªØµØ§Ø¯ Ù…Ù†Ø§Ø¨Ø¹ \n",
            "10) Ø§Ù‚ØªØµØ§Ø¯ Ø¨ÙŠÙ…Ù‡ \n",
            "11) Ø§Ù‚ØªØµØ§Ø¯ Ù†Ù‡Ø§Ø¯Ú¯Ø±Ø§ \n",
            "12) Ø§Ù‚ØªØµØ§Ø¯ Ø±ÙŠØ§Ø¶ÙŠ \n",
            "13) Ø§Ù‚ØªØµØ§Ø¯ Ø§ÙŠØ±Ø§Ù†\n",
            "\n",
            "==================================================\n",
            "[Ø´Ù…Ø§]: Ù†Ù…Ø±Ù‡ Ú©Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ú†Ú¯ÙˆÙ†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒ Ø´ÙˆØ¯ØŸ\n",
            "... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\n",
            "... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\n",
            "\n",
            "[Ø±Ø¨Ø§Øª]:\n",
            "Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ ÛŒØ§ÙØª Ù†Ø´Ø¯.\n",
            "\n",
            "==================================================\n",
            "[Ø´Ù…Ø§]: Ø¶Ø±ÛŒØ¨ ØªØ§Ø«ÛŒØ± Ø³ÙˆØ§Ø¨Ù‚ Ø¢Ù…ÙˆØ²Ø´ÛŒØŒ Ù¾Ú˜ÙˆÙ‡Ø´ÛŒ Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒ Ú†Ù†Ø¯ Ø¯Ø±ØµØ¯ Ø§Ø³ØªØŸ\n",
            "... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\n",
            "... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\n",
            "\n",
            "[Ø±Ø¨Ø§Øª]:\n",
            "30 Ø¯Ø±ØµØ¯\n",
            "\n",
            "==================================================\n",
            "[Ø´Ù…Ø§]: Ú©Ø¯ Ø±Ø´ØªÙ‡ Ù…Ø­Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø±Ø´ØªÙ‡ Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± - Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ú©Ø¯Ø§Ù…Ù†Ø¯ØŸ\n",
            "... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\n",
            "... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\n",
            "\n",
            "[Ø±Ø¨Ø§Øª]:\n",
            "Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù….\n",
            "\n",
            "==================================================\n",
            "[Ø´Ù…Ø§]: Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§ØµÙ„Ø§Ø­ÛŒÙ‡ØŒ ØªØ§Ø±ÛŒØ® Ø¯Ù‚ÛŒÙ‚ Ø¨Ø±Ø±Ø³ÛŒ Ø³ÙˆØ§Ø¨Ù‚ Ø¹Ù„Ù…ÛŒ Ùˆ Ù…ØµØ§Ø­Ø¨Ù‡ Ú†Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø§Ø³ØªØŸ\n",
            "... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\n",
            "... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\n",
            "\n",
            "[Ø±Ø¨Ø§Øª]:\n",
            "Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù….\n",
            "\n",
            "==================================================\n",
            "[Ø´Ù…Ø§]: Ø§Ø³Ù… Ù…Ù† Ú†ÛŒØ³ØªØŸ\n",
            "... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\n",
            "... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\n",
            "\n",
            "[Ø±Ø¨Ø§Øª]:\n",
            "Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù….\n",
            "\n",
            "==================================================\n",
            "[Ø´Ù…Ø§]: Ù¾Ø§ÛŒØªØ®Øª Ø§ÛŒØ±Ø§Ù† Ú©Ø¬Ø§Ø³ØªØŸ\n",
            "... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\n",
            "... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\n",
            "\n",
            "[Ø±Ø¨Ø§Øª]:\n",
            "Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù….\n",
            "\n",
            "==================================================\n",
            "--- ğŸ ØªØ³Øª Ø±Ø¨Ø§Øª ØªÙ…Ø§Ù… Ø´Ø¯ ğŸ ---\n"
          ]
        }
      ],
      "source": [
        "# --- Ø³Ù„ÙˆÙ„ Û¸ (Ù†Ù‡Ø§ÛŒÛŒ): Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ø³ÙˆØ§Ù„Ø§Øª ØªØ³ØªÛŒ Ùˆ ÙÙ†ÛŒ ---\n",
        "\n",
        "print(\"--- ğŸ¤– Ø´Ø±ÙˆØ¹ ØªØ³Øª Ø±Ø¨Ø§Øª Ø¨Ø§ Ø³ÙˆØ§Ù„Ø§Øª Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ ğŸ¤– ---\")\n",
        "\n",
        "# --- Ø¯Ø³ØªÙ‡ Û±: Ø³ÙˆØ§Ù„Ø§Øª Ø§ØµÙ„ÛŒ Ø´Ù…Ø§ ---\n",
        "\n",
        "query_1 = \"Ø³Ù‡Ù…ÛŒÙ‡ Ø§ÛŒØ«Ø§Ø±Ú¯Ø±Ø§Ù† Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ\"\n",
        "print(f\"[Ø´Ù…Ø§]: {query_1}\")\n",
        "answer_1 = ask_robot(query_1)\n",
        "print(f\"\\n[Ø±Ø¨Ø§Øª]:\\n{answer_1}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "query_2 = \"Ú¯Ø±Ø§ÛŒØ´ Ù‡Ø§ÛŒ Ø±Ø´ØªÙ‡ Ø²Ø¨Ø§Ù† Ùˆ Ø§Ø¯Ø¨ÛŒØ§Øª ÙØ§Ø±Ø³ÛŒØŸ\"\n",
        "print(f\"[Ø´Ù…Ø§]: {query_2}\")\n",
        "answer_2 = ask_robot(query_2)\n",
        "print(f\"\\n[Ø±Ø¨Ø§Øª]:\\n{answer_2}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "query_3 = \"Ú¯Ø±Ø§ÛŒØ´ Ù‡Ø§ÛŒ Ø¹Ù„ÙˆÙ… Ø§Ù‚ØªØµØ§Ø¯ÙŠØŸ\"\n",
        "print(f\"[Ø´Ù…Ø§]: {query_3}\")\n",
        "answer_3 = ask_robot(query_3)\n",
        "print(f\"\\n[Ø±Ø¨Ø§Øª]:\\n{answer_3}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# --- Ø¯Ø³ØªÙ‡ Û²: Ø³ÙˆØ§Ù„Ø§Øª ÙÙ†ÛŒ Ùˆ Ø¯Ù‚ÛŒÙ‚ (Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù‚Ø¯Ø±Øª Ø¬Ø³ØªØ¬Ùˆ) ---\n",
        "\n",
        "query_4 = \"Ù†Ù…Ø±Ù‡ Ú©Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ú†Ú¯ÙˆÙ†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒ Ø´ÙˆØ¯ØŸ\"\n",
        "print(f\"[Ø´Ù…Ø§]: {query_4}\")\n",
        "answer_4 = ask_robot(query_4)\n",
        "print(f\"\\n[Ø±Ø¨Ø§Øª]:\\n{answer_4}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "query_5 = \"Ø¶Ø±ÛŒØ¨ ØªØ§Ø«ÛŒØ± Ø³ÙˆØ§Ø¨Ù‚ Ø¢Ù…ÙˆØ²Ø´ÛŒØŒ Ù¾Ú˜ÙˆÙ‡Ø´ÛŒ Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒ Ú†Ù†Ø¯ Ø¯Ø±ØµØ¯ Ø§Ø³ØªØŸ\"\n",
        "print(f\"[Ø´Ù…Ø§]: {query_5}\")\n",
        "answer_5 = ask_robot(query_5)\n",
        "print(f\"\\n[Ø±Ø¨Ø§Øª]:\\n{answer_5}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# --- Ø¯Ø³ØªÙ‡ Û³: Ø³ÙˆØ§Ù„Ø§Øª ØªØ±Ú©ÛŒØ¨ÛŒ (Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø±Ú© Ø§ØµÙ„Ø§Ø­ÛŒÙ‡) ---\n",
        "\n",
        "query_6 = \"Ú©Ø¯ Ø±Ø´ØªÙ‡ Ù…Ø­Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø±Ø´ØªÙ‡ Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± - Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ú©Ø¯Ø§Ù…Ù†Ø¯ØŸ\"\n",
        "print(f\"[Ø´Ù…Ø§]: {query_6}\")\n",
        "answer_6 = ask_robot(query_6)\n",
        "print(f\"\\n[Ø±Ø¨Ø§Øª]:\\n{answer_6}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "query_7 = \"Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§ØµÙ„Ø§Ø­ÛŒÙ‡ØŒ ØªØ§Ø±ÛŒØ® Ø¯Ù‚ÛŒÙ‚ Ø¨Ø±Ø±Ø³ÛŒ Ø³ÙˆØ§Ø¨Ù‚ Ø¹Ù„Ù…ÛŒ Ùˆ Ù…ØµØ§Ø­Ø¨Ù‡ Ú†Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø§Ø³ØªØŸ\"\n",
        "print(f\"[Ø´Ù…Ø§]: {query_7}\")\n",
        "answer_7 = ask_robot(query_7)\n",
        "print(f\"\\n[Ø±Ø¨Ø§Øª]:\\n{answer_7}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# --- Ø¯Ø³ØªÙ‡ Û´: Ø³ÙˆØ§Ù„Ø§Øª Ú©Ù†ØªØ±Ù„ÛŒ (Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø§ÛŒÙ…Ù†ÛŒ Ø±Ø¨Ø§Øª) ---\n",
        "\n",
        "query_8 = \"Ø§Ø³Ù… Ù…Ù† Ú†ÛŒØ³ØªØŸ\"\n",
        "print(f\"[Ø´Ù…Ø§]: {query_8}\")\n",
        "answer_8 = ask_robot(query_8)\n",
        "print(f\"\\n[Ø±Ø¨Ø§Øª]:\\n{answer_8}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "query_9 = \"Ù¾Ø§ÛŒØªØ®Øª Ø§ÛŒØ±Ø§Ù† Ú©Ø¬Ø§Ø³ØªØŸ\"\n",
        "print(f\"[Ø´Ù…Ø§]: {query_9}\")\n",
        "answer_9 = ask_robot(query_9)\n",
        "print(f\"\\n[Ø±Ø¨Ø§Øª]:\\n{answer_9}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"--- ğŸ ØªØ³Øª Ø±Ø¨Ø§Øª ØªÙ…Ø§Ù… Ø´Ø¯ ğŸ ---\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPienPZZaCedg8JoRXnFyQy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}