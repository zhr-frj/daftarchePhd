{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhr-frj/daftarchePhd/blob/main/Copy_of_Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W0CPCljcr5F",
        "outputId": "8d1da7d3-7be2-4372-e32f-6a3ae308af81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Nov 12 09:48:45 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPWK7nYxcGPx",
        "outputId": "c57a62e8-b5c0-4cad-b4de-ed942eabc8d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ ---\n",
            "--- Ù†ØµØ¨ Ú©Ø§Ù…Ù„ Ø´Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- Ø³Ù„ÙˆÙ„ Û±: Ù†ØµØ¨ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ùˆ Ù¾Ø§ÛŒØ¯Ø§Ø± ---\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ ---\")\n",
        "!pip install langchain langchain-community transformers bitsandbytes accelerate faiss-cpu pypdf sentence-transformers langchain_huggingface docarray -q\n",
        "print(\"--- Ù†ØµØ¨ Ú©Ø§Ù…Ù„ Ø´Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iUDnauFYxFq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a594176-edb9-47e4-c08e-5c885440f40f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ (Quantization) ---\n",
            "--- Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ---\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from transformers import BitsAndBytesConfig\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ (Quantization) ---\")\n",
        "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ 4-bit Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "print(\"--- Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZrAJvITztf9",
        "outputId": "cff85f8f-cd55-43b6-8b0d-7f0cd175cf26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØªØ±Ø¬Ù…Â» (Gemma 2B) ---\n",
            "--- Â«Ù…ØªØ±Ø¬Ù…Â» (Tokenizer) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# # --- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØªØ±Ø¬Ù…Â» (Tokenizer) ---\n",
        "# print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØªØ±Ø¬Ù…Â» (Tokenizer) ---\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"unsloth/gemma-2-9b-it\", use_fast=True)\n",
        "# print(\"--- Â«Ù…ØªØ±Ø¬Ù…Â» (Tokenizer) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\")\n",
        "\n",
        "\n",
        "# --- Ø³Ù„ÙˆÙ„ Û².Û²: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØªØ±Ø¬Ù…Â» (Tokenizer) - Ù†Ø³Ø®Ù‡ Ø³Ø¨Ú© Gemma 2B ---\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØªØ±Ø¬Ù…Â» (Gemma 2B) ---\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"unsloth/gemma-2-2b-it\",\n",
        "    use_fast=True\n",
        "    )\n",
        "print(\"--- Â«Ù…ØªØ±Ø¬Ù…Â» (Tokenizer) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TihILDUIzx7o",
        "outputId": "237393c1-654c-4a2c-d179-8c314da905ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØºØ²Â» (LLM) --- (Ù†Ø³Ø®Ù‡ Ø³Ø¨Ú© Gemma 2B)\n",
            "--- Â«Ù…ØºØ²Â» (LLM) Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# # --- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØºØ²Â» (LLM) ---\n",
        "# print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØºØ²Â» (LLM) --- (Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ø¨Ø³ÛŒØ§Ø± Ø³Ù†Ú¯ÛŒÙ† Ùˆ Ø²Ù…Ø§Ù†â€ŒØ¨Ø± Ø§Ø³Øª)\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"unsloth/gemma-2-9b-it\",\n",
        "#     device_map=\"auto\",\n",
        "#     quantization_config=bnb_config,\n",
        "#     torch_dtype=torch.float16\n",
        "# )\n",
        "# print(\"--- Â«Ù…ØºØ²Â» (LLM) Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Ø³Ù„ÙˆÙ„ Û².Û³: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØºØ²Â» (LLM) - Ù†Ø³Ø®Ù‡ Ø³Ø¨Ú© Gemma 2B ---\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ù…ØºØ²Â» (LLM) --- (Ù†Ø³Ø®Ù‡ Ø³Ø¨Ú© Gemma 2B)\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"unsloth/gemma-2-2b-it\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config, # Ø§Ø² bnb_config Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "print(\"--- Â«Ù…ØºØ²Â» (LLM) Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aXgzl26z5Br",
        "outputId": "d945c7ba-dc82-4b71-fd18-faea2cac0802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª Â«Ø®Ø· Ù„ÙˆÙ„Ù‡Â» (Pipeline) ---\n",
            "--- Â«Ø®Ø· Ù„ÙˆÙ„Ù‡Â»ØŒ Â«ØºÙ„Ø§ÙÂ» Ùˆ Â«ØªÙ…ÛŒØ²Ú©Ù†Ù†Ø¯Ù‡Â» Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- Ø³Ø§Ø®Øª Â«Ø®Ø· Ù„ÙˆÙ„Ù‡Â» Ùˆ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ LangChain ---\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª Â«Ø®Ø· Ù„ÙˆÙ„Ù‡Â» (Pipeline) ---\")\n",
        "text_gen_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512  # Ø§ÙØ²Ø§ÛŒØ´ Ø·ÙˆÙ„ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ø¬ÙˆØ§Ø¨â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„â€ŒØªØ±\n",
        ")\n",
        "\n",
        "# Ø³Ø§Ø®Øª Â«ØºÙ„Ø§ÙÂ» LangChain Ø¨Ø±Ø§ÛŒ LLM\n",
        "llm = HuggingFacePipeline(pipeline=text_gen_pipeline)\n",
        "\n",
        "# Ø³Ø§Ø®Øª Â«ØªÙ…ÛŒØ²Ú©Ù†Ù†Ø¯Ù‡ Ø®Ø±ÙˆØ¬ÛŒÂ»\n",
        "parser = StrOutputParser()\n",
        "\n",
        "print(\"--- Â«Ø®Ø· Ù„ÙˆÙ„Ù‡Â»ØŒ Â«ØºÙ„Ø§ÙÂ» Ùˆ Â«ØªÙ…ÛŒØ²Ú©Ù†Ù†Ø¯Ù‡Â» Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6wHqoJq0TiM",
        "outputId": "16f57507-b8fa-4d5b-84e4-aa928b0133cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Â«Ù‚Ø§Ù„Ø¨ Ú†ØªÂ» Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¹Ù…Ø§Ù„ Â«Ù‚Ø§Ù„Ø¨ Ú†ØªÂ» ---\n",
        "\n",
        "# Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ØŒ Ù¾Ø±Ø§Ù…Ù¾Øª Ù†Ù‡Ø§ÛŒÛŒ Ù…Ø§ Ø±Ø§ Ø¯Ø± Ù‚Ø§Ù„Ø¨ Ú†Øª Ø±Ø³Ù…ÛŒ Ù…Ø¯Ù„ Gemma Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡Ø¯\n",
        "def apply_chat_template_and_response(prompt):\n",
        "    messages = [\n",
        "    {'role': 'user', 'content': prompt}\n",
        "    ]\n",
        "\n",
        "    # Ø§Ø¹Ù…Ø§Ù„ Ù‚Ø§Ù„Ø¨ Ú†Øª\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False\n",
        "    )\n",
        "\n",
        "    # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…Ø¯Ù„ Ùˆ ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ Ø§ÙˆÙ„ÛŒÙ‡\n",
        "    response = llm.invoke(text)\n",
        "    # response Ø±Ø§ ØªÙ…ÛŒØ² Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ ÙÙ‚Ø· Ø¬ÙˆØ§Ø¨ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§Ù‚ÛŒ Ø¨Ù…Ø§Ù†Ø¯\n",
        "    return response.replace(text, '').strip()\n",
        "\n",
        "print(\"--- ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Â«Ù‚Ø§Ù„Ø¨ Ú†ØªÂ» Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-oLghHF0WSw",
        "outputId": "4ba5fe41-8935-4525-b732-91ecd0765acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name sbunlp/fabert. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ø¨ÛŒÙ†ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒÙÙ‡Ù…Â» (Embeddings) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at sbunlp/fabert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Â«Ø¨ÛŒÙ†ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒÙÙ‡Ù…Â» (Embeddings) Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ø¨ÛŒÙ†ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒÙÙ‡Ù…Â» (Embedding Model) ---\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Â«Ø¨ÛŒÙ†ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒÙÙ‡Ù…Â» (Embeddings) ---\")\n",
        "# Ù…Ø§ Ø§Ø² ÛŒÚ© Ù…Ø¯Ù„ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ ÙØ§Ø±Ø³ÛŒ Ù‚ÙˆÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n",
        "embedding_name = 'sbunlp/fabert'\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_name,\n",
        "    model_kwargs={'device': 'cpu'} # Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ Ø±Ø§ Ø±ÙˆÛŒ CPU Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø­Ø§ÙØ¸Ù‡ GPU Ø¨Ø±Ø§ÛŒ LLM Ø¢Ø²Ø§Ø¯ Ø¨Ù…Ø§Ù†Ø¯\n",
        ")\n",
        "print(\"--- Â«Ø¨ÛŒÙ†ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒÙÙ‡Ù…Â» (Embeddings) Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMM1J-0N0hCQ",
        "outputId": "671bc5d6-832c-409e-b2ca-1fa656f2bcda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Â«Ø¨Ø±Ú¯Ù‡ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„Â» (Prompt) Ù…Ø®ØµÙˆØµ Gemma Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# # --- ØªØ¹Ø±ÛŒÙ Â«Ø¨Ø±Ú¯Ù‡ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„Â» (Prompt Template) ---\n",
        "\n",
        "# from langchain.prompts import PromptTemplate\n",
        "\n",
        "# # Ø§ÛŒÙ† Ø§Ù„Ú¯Ùˆ Ø¨Ù‡ Ø±Ø¨Ø§Øª Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯ Ú©Ù‡ Â«Ø®Ø§Ø±Ø¬ Ø§Ø² PDFÂ» Ø¬ÙˆØ§Ø¨ Ù†Ø¯Ù‡Ø¯\n",
        "# template = \"\"\"\n",
        "# Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ø¨Ø³ÛŒØ§Ø± Ø¯Ù‚ÛŒÙ‚ Ù‡Ø³ØªÛŒØ¯. ÙˆØ¸ÛŒÙÙ‡ Ø´Ù…Ø§ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙ†ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø¨Ø®Ø´ Â«Ø²Ù…ÛŒÙ†Ù‡Â» Ø¨Ù‡ Ø´Ù…Ø§ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "# Ø¨Ù‡ Ù‡ÛŒÚ† ÙˆØ¬Ù‡ Ø§Ø² Ø¯Ø§Ù†Ø´ Ù‚Ø¨Ù„ÛŒ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯.\n",
        "# Ù¾Ø§Ø³Ø® Ø±Ø§ ÙÙ‚Ø· Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.\n",
        "# Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø³ÙˆØ§Ù„ Ø¯Ø± Â«Ø²Ù…ÛŒÙ†Ù‡Â» ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªØŒ Ø¨Ù‡ Ø³Ø§Ø¯Ú¯ÛŒ Ø¨Ú¯ÙˆÛŒÛŒØ¯: \"Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù….\"\n",
        "\n",
        "# Ø²Ù…ÛŒÙ†Ù‡ (Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡ Ø§Ø² PDF):\n",
        "# {context}\n",
        "\n",
        "# Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±:\n",
        "# {question}\n",
        "\n",
        "# Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ø´Ù…Ø§:\n",
        "# \"\"\"\n",
        "\n",
        "# prompt = PromptTemplate.from_template(template)\n",
        "# print(\"--- Â«Ø¨Ø±Ú¯Ù‡ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„Â» (Prompt) Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ---\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Ø³Ù„ÙˆÙ„ Ûµ: ØªØ¹Ø±ÛŒÙ Â«Ø¨Ø±Ú¯Ù‡ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„Â» (Prompt Template) - Ù†Ø³Ø®Ù‡ Gemma ---\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Ù‚Ø§Ù„Ø¨ Ù¾Ø±Ø§Ù…Ù¾Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ (Ø¨Ø¯ÙˆÙ† ØªÚ¯â€ŒÙ‡Ø§ÛŒ <|user|>)\n",
        "template = \"\"\"\n",
        "Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ø¨Ø³ÛŒØ§Ø± Ø¯Ù‚ÛŒÙ‚ Ù‡Ø³ØªÛŒØ¯. ÙˆØ¸ÛŒÙÙ‡ Ø´Ù…Ø§ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙ†ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø¨Ø®Ø´ Â«Ø²Ù…ÛŒÙ†Ù‡Â» Ø¨Ù‡ Ø´Ù…Ø§ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "Ø¨Ù‡ Ù‡ÛŒÚ† ÙˆØ¬Ù‡ Ø§Ø² Ø¯Ø§Ù†Ø´ Ù‚Ø¨Ù„ÛŒ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯.\n",
        "Ù¾Ø§Ø³Ø® Ø±Ø§ ÙÙ‚Ø· Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.\n",
        "Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø³ÙˆØ§Ù„ Ø¯Ø± Â«Ø²Ù…ÛŒÙ†Ù‡Â» ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªØŒ Ø¨Ù‡ Ø³Ø§Ø¯Ú¯ÛŒ Ø¨Ú¯ÙˆÛŒÛŒØ¯: \"Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù….\"\n",
        "\n",
        "Ø²Ù…ÛŒÙ†Ù‡ (Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡ Ø§Ø² PDF):\n",
        "{context}\n",
        "\n",
        "Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±:\n",
        "{question}\n",
        "\n",
        "Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ø´Ù…Ø§:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "print(\"--- Â«Ø¨Ø±Ú¯Ù‡ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„Â» (Prompt) Ù…Ø®ØµÙˆØµ Gemma Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWOmShtw0iBv",
        "outputId": "0f121c96-3057-45ee-8444-543ee263016d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ PDF ---\n",
            "--- ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ PDF ---\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ PDF ---\")\n",
        "!wget -q \"https://github.com/zhr-frj/daftarchePhd/releases/download/v1.0.0/Eslahiye-PHD1405-.konkur.in.pdf\" -O daftarche_asli.pdf\n",
        "!wget -q \"https://github.com/zhr-frj/daftarchePhd/releases/download/v1.0.0/Phd1405-.konkur.in.pdf\" -O daftarche_eslahiye.pdf\n",
        "print(\"--- ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97d3gHt71QfS",
        "outputId": "f83100fc-0d35-4b4f-ada8-299988686bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ PDF\n",
            "--- Ù…Ø¬Ù…ÙˆØ¹Ø§ 58 ØµÙØ­Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\n",
            "... Ø¯Ø± Ø­Ø§Ù„ Ø®ÙØ±Ø¯ Ú©Ø±Ø¯Ù† Ù…ØªÙ†â€ŒÙ‡Ø§\n",
            "--- Ù…ØªÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ 206 ØªÚ©Ù‡ (Chunk) ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯Ù†Ø¯ ---\n"
          ]
        }
      ],
      "source": [
        "# --- Ø®ÙˆØ§Ù†Ø¯Ù†ØŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø®ÙØ±Ø¯ Ú©Ø±Ø¯Ù† Ù…ØªÙ†â€ŒÙ‡Ø§ ---\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Û±. Ø®ÙˆØ§Ù†Ø¯Ù† Ù‡Ø± Ø¯Ùˆ ÙØ§ÛŒÙ„ PDF\n",
        "print(\"... Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ PDF\")\n",
        "loader_asli = PyPDFLoader(\"daftarche_asli.pdf\")\n",
        "loader_eslahiye = PyPDFLoader(\"daftarche_eslahiye.pdf\")\n",
        "all_pages = loader_asli.load() + loader_eslahiye.load()\n",
        "print(f\"--- Ù…Ø¬Ù…ÙˆØ¹Ø§ {len(all_pages)} ØµÙØ­Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ ---\")\n",
        "\n",
        "# Û². Ø®ÙØ±Ø¯ Ú©Ø±Ø¯Ù† Ù…ØªÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ ØªÚ©Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú© (Chunks)\n",
        "print(\"... Ø¯Ø± Ø­Ø§Ù„ Ø®ÙØ±Ø¯ Ú©Ø±Ø¯Ù† Ù…ØªÙ†â€ŒÙ‡Ø§\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)\n",
        "text_documents = text_splitter.split_documents(all_pages)\n",
        "print(f\"--- Ù…ØªÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ {len(text_documents)} ØªÚ©Ù‡ (Chunk) ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯Ù†Ø¯ ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKEaDg631UKF",
        "outputId": "d947d6ce-5c4d-4eab-b886-536494352f26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» (Vectorstore)... (Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ø²Ù…Ø§Ù†â€ŒØ¨Ø± Ø§Ø³Øª)\n",
            "--- Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯ ---\n",
            "--- Â«Ú©ØªØ§Ø¨Ø¯Ø§Ø±Â» (Retriever) Ø¢Ù…Ø§Ø¯Ù‡ Ø¬Ø³ØªØ¬Ùˆ Ø§Ø³Øª ---\n"
          ]
        }
      ],
      "source": [
        "# --- Ø³Ø§Ø®Øª Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» Ùˆ Â«Ú©ØªØ§Ø¨Ø¯Ø§Ø±Â» ---\n",
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "# Û±. Ø³Ø§Ø®Øª Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» (Vectorstore)\n",
        "# (Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ ØªÙ…Ø§Ù… ØªÚ©Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ÙˆÚ©ØªÙˆØ± ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø³Ù†Ú¯ÛŒÙ†â€ŒØªØ±ÛŒÙ† Ø¨Ø®Ø´ Ø§Ø³Øª)\n",
        "print(\"... Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» (Vectorstore)... (Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ø²Ù…Ø§Ù†â€ŒØ¨Ø± Ø§Ø³Øª)\")\n",
        "vectorstore = DocArrayInMemorySearch.from_documents(\n",
        "    text_documents,\n",
        "    embedding=embeddings # Ø§Ø² Ù…ØªØºÛŒØ± embeddings Ú©Ù‡ Ø¯Ø± Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø³Ø§Ø®ØªÛŒÙ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
        ")\n",
        "print(\"--- Â«Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Â» Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯ ---\")\n",
        "\n",
        "# Û². Ø³Ø§Ø®Øª Â«Ú©ØªØ§Ø¨Ø¯Ø§Ø±Â» (Retriever)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={'k': 3}) # Û³ Ø³Ù†Ø¯ Ù…Ø±ØªØ¨Ø· Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†\n",
        "print(\"--- Â«Ú©ØªØ§Ø¨Ø¯Ø§Ø±Â» (Retriever) Ø¢Ù…Ø§Ø¯Ù‡ Ø¬Ø³ØªØ¬Ùˆ Ø§Ø³Øª ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGmdoWAC1Yvr",
        "outputId": "37fb4ff3-f7d3-4c42-8b90-3fcb04a29d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ø¯Ø± Ø­Ø§Ù„ ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ 'ask_robot' ---\n",
            "--- ØªØ§Ø¨Ø¹ 'ask_robot' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª ---\n"
          ]
        }
      ],
      "source": [
        "# --- ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ Ù†Ù‡Ø§ÛŒÛŒ Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø® ---\n",
        "\n",
        "print(\"--- Ø¯Ø± Ø­Ø§Ù„ ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ 'ask_robot' ---\")\n",
        "\n",
        "def ask_robot(question):\n",
        "    \"\"\"\n",
        "    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ÛŒÚ© Ø³ÙˆØ§Ù„ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ùˆ ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ RAG Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\")\n",
        "\n",
        "        # Û±. Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÚ©Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ† Ù…Ø±ØªØ¨Ø· (Retrieve)\n",
        "        retrieved_context_docs = retriever.invoke(question)\n",
        "        # ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ³Øª Ø§Ø³Ù†Ø§Ø¯ Ø¨Ù‡ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø­Ø¯\n",
        "        retrieved_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_context_docs)\n",
        "\n",
        "        print(\"... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\")\n",
        "\n",
        "        # Û². Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Â«Ø²Ù…ÛŒÙ†Ù‡Â» Ùˆ Â«Ø³ÙˆØ§Ù„Â» (Augment)\n",
        "        formatted_prompt = prompt.format(context=retrieved_context, question=question)\n",
        "\n",
        "        # Û³. Ø§Ø±Ø³Ø§Ù„ Ù¾Ø±Ø§Ù…Ù¾Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ù‡ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¬ÙˆØ§Ø¨ (Generate)\n",
        "        response_from_model = apply_chat_template_and_response(formatted_prompt)\n",
        "\n",
        "        # Û´. ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ø¬ÙˆØ§Ø¨\n",
        "        parsed_response = parser.parse(response_from_model)\n",
        "        return parsed_response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø±Ø® Ø¯Ø§Ø¯: {e}\"\n",
        "\n",
        "print(\"--- ØªØ§Ø¨Ø¹ 'ask_robot' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0c8g3f02QO3",
        "outputId": "6ed5b0a5-9850-4661-ec6c-bac55af74be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ø´Ù…Ø§]: Ø³Ù‡Ù…ÛŒÙ‡ Ø§ÛŒØ«Ø§Ø±Ú¯Ø±Ø§Ù† Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ\n",
            "... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\n",
            "... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\n",
            "\n",
            "[Ø±Ø¨Ø§Øª]:\n",
            "Ø³Ø§Ú©Ù†Ø§Ù† Ø¯Ø± Ù…ØªÙ†ØŒ Ø³Ù‡Ù…ÛŒÙ‡ Ø§ÛŒØ«Ø§Ø±Ú¯Ø±Ø§Ù† Ø¨Ù‡ Ø´Ø±Ø­ Ø²ÛŒØ± Ø§Ø³Øª: \n",
            "\n",
            "* Ø¬Ø§Ù†Ø¨Ø§Ø² 52% Ùˆ Ø¨Ø§Ù„Ø§ØªØ±: 52%\n",
            "* Ø¬Ø§Ù†Ø¨Ø§Ø² 52% Ùˆ Ú©Ù…ØªØ± Ùˆ Ù‡Ù…Ø³Ø± Ùˆ ÙØ±Ø²Ù†Ø¯Ø§Ù† Ø¢Ù†Ø§Ù†: 5% \n",
            "* Ø¢Ø²Ø§Ø¯Ú¯Ø§Ù† Ùˆ Ù‡Ù…Ø³Ø± Ùˆ ÙØ±Ø²Ù†Ø¯Ø§Ù† Ø¢Ù†Ø§Ù†: 5%\n",
            "* Ù‡Ù…Ø³Ø± Ùˆ ÙØ±Ø²Ù†Ø¯Ø§Ù† Ø±Ø²Ù…Ù†Ø¯Ú¯Ø§Ù† Ø¨Ø§ Ø­Ø¯Ø§Ù‚Ù„ Ø´Ø´ Ù…Ø§Ù‡ Ø­Ø¶ÙˆØ± Ø¯Ø§ÙˆØ·Ù„Ø¨Ø§Ù†Ù‡ Ø¯Ø± Ø¬Ø¨Ù‡Ù‡: 5%\n",
            "* Ù‡Ù…Ø³Ø± Ùˆ ÙØ±Ø²Ù†Ø¯Ø§Ù† Ø´Ù‡Ø¯Ø§ Ùˆ Ù…ÙÙ‚ÙˆØ¯Ø§Ù„Ø§Ø«Ø±Ø§Ù†: 5%\n",
            "\n",
            "==================================================\n",
            "[Ø´Ù…Ø§]: Ú¯Ø±Ø§ÛŒØ´ Ù‡Ø§ÛŒ Ø±Ø´ØªÙ‡ Ø²Ø¨Ø§Ù† Ùˆ Ø§Ø¯Ø¨ÛŒØ§Øª ÙØ§Ø±Ø³ÛŒØŸ\n",
            "... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\n",
            "... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Ø±Ø¨Ø§Øª]:\n",
            "Ø±Ø´ØªÙ‡ Ú¯Ø±Ø§ÙŠØ´ \n",
            "Ú©Ø¯ \n",
            "ØªØ±Ú©ÙŠØ¨  \n",
            "ØªØ±Ú©ÙŠØ¨ Ø¯Ø±ÙˆØ³ Ø§Ù…ØªØ­Ø§Ù†ÙŠ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø±ÙˆØ³ \n",
            "1 2 3  \n",
            "Ø¢Ù…ÙˆØ²Ø´ Ø²Ø¨Ø§Ù† Ùˆ Ø§Ø¯Ø¨ÙŠØ§Øª ÙØ§Ø±Ø³ÛŒ  - 1 ï \n",
            "Ø±Ø´ØªÙ‡Ù‡Ø§ÙŠ Ù…Ø±ØªØ¨Ø· Ú©Ù‡ ÙØ§Ø±ØºØ§Ù„ØªØ­ØµÙŠÙ„Ø§Ù† Ø¢Ù† Ù…ÙŠØªÙˆØ§Ù†Ù†Ø¯ Ø¯Ø± Ø§ÙŠÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§Ù…ØªØ­Ø§Ù†ÙŠ Ø´Ø±Ú©Øª Ú©Ù†Ù†Ø¯: \n",
            "Ø²Ø¨Ø§Ù† Ùˆ Ø§Ø¯Ø¨ÙŠØ§Øª ÙØ§Ø±Ø³ÛŒØŒ Ø¢Ù…ÙˆØ²Ø´ Ø²Ø¨Ø§Ù† Ùˆ Ø§Ø¯Ø¨ÙŠØ§Øª ÙØ§Ø±Ø³ÛŒØŒ Ù…ØªØ±Ø¬Ù…ÙŠ Ø²Ø¨Ø§Ù† Ùˆ Ø§Ø¯Ø¨ÙŠØ§Øª ÙØ§Ø±Ø³ÛŒØŒ Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ø´Ø¹Ø± Ùˆ Ø¯Ø§Ø³ØªØ§Ù†\n",
            "\n",
            "==================================================\n",
            "[Ø´Ù…Ø§]: Ú¯Ø±Ø§ÛŒØ´ Ù‡Ø§ÛŒ Ø¹Ù„ÙˆÙ… Ø§Ù‚ØªØµØ§Ø¯ÙŠØŸ\n",
            "... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\n",
            "... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\n",
            "\n",
            "[Ø±Ø¨Ø§Øª]:\n",
            "* Ø§Ù‚ØªØµØ§Ø¯Ø³Ù†Ø¬ÙŠ\n",
            "* Ø§Ù‚ØªØµØ§Ø¯ Ù¾ÙˆÙ„ÙŠ \n",
            "* Ø§Ù‚ØªØµØ§Ø¯ Ø§Ø³Ù„Ø§Ù…ÙŠ \n",
            "* Ø§Ù‚ØªØµØ§Ø¯ Ø¨Ø®Ø´ Ø¹Ù…ÙˆÙ…ÙŠ \n",
            "* ØªÙˆØ³Ø¹Ù‡ Ø§Ù‚ØªØµØ§Ø¯ÙŠ \n",
            "* Ø§Ù‚ØªØµØ§Ø¯ Ø¨ÙŠÙ†Ø§Ù„Ù…Ù„Ù„ \n",
            "* Ø§Ù‚ØªØµØ§Ø¯ Ù…Ø§Ù„ÙŠ \n",
            "* Ø§Ù‚ØªØµØ§Ø¯ Ø´Ù‡Ø±ÙŠ Ùˆ Ù…Ù†Ø·Ù‚Ù‡Ø§ÙŠ \n",
            "* Ø§Ù‚ØªØµØ§Ø¯ Ù…Ù†Ø§Ø¨Ø¹ \n",
            "* Ø§Ù‚ØªØµØ§Ø¯ Ø¨ÙŠÙ…Ù‡ \n",
            "* Ø§Ù‚ØªØµØ§Ø¯ Ù†Ù‡Ø§Ø¯Ú¯Ø±Ø§ \n",
            "* Ø§Ù‚ØªØµØ§Ø¯ Ø±ÙŠØ§Ø¶ÙŠ \n",
            "* Ø§Ù‚ØªØµØ§Ø¯ Ø§ÙŠØ±Ø§Ù†\n",
            "\n",
            "==================================================\n",
            "[Ø´Ù…Ø§]: Ø§Ø³Ù… Ù…Ù† Ú†ÛŒÙ‡ØŸ\n",
            "... ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÙØªØ±Ú†Ù‡â€ŒÙ‡Ø§ ...\n",
            "... ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® ...\n",
            "\n",
            "[Ø±Ø¨Ø§Øª]:\n",
            "Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù….\n"
          ]
        }
      ],
      "source": [
        "# --- Ø³Ù„ÙˆÙ„ Û¸: Ø§Ø² Ø±Ø¨Ø§Øª Ø¨Ù¾Ø±Ø³ÛŒØ¯! ---\n",
        "\n",
        "# Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯\n",
        "query = \"Ø³Ù‡Ù…ÛŒÙ‡ Ø§ÛŒØ«Ø§Ø±Ú¯Ø±Ø§Ù† Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ\"\n",
        "\n",
        "# --- Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ ---\n",
        "print(f\"[Ø´Ù…Ø§]: {query}\")\n",
        "answer = ask_robot(query)\n",
        "print(f\"\\n[Ø±Ø¨Ø§Øª]:\\n{answer}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø³ÛŒØ¯Ù† Ø³ÙˆØ§Ù„ Ø¬Ø¯ÛŒØ¯ØŒ ÙÙ‚Ø· Ù…ØªÙ† query Ø±Ø§ Ø¹ÙˆØ¶ Ú©Ù†ÛŒØ¯ Ùˆ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù‡Ù…ÛŒÙ† Ø³Ù„ÙˆÙ„ Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯\n",
        "query_2 = \"Ú¯Ø±Ø§ÛŒØ´ Ù‡Ø§ÛŒ Ø±Ø´ØªÙ‡ Ø²Ø¨Ø§Ù† Ùˆ Ø§Ø¯Ø¨ÛŒØ§Øª ÙØ§Ø±Ø³ÛŒØŸ\"\n",
        "print(f\"[Ø´Ù…Ø§]: {query_2}\")\n",
        "answer_2 = ask_robot(query_2)\n",
        "print(f\"\\n[Ø±Ø¨Ø§Øª]:\\n{answer_2}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "query_3 = \"Ú¯Ø±Ø§ÛŒØ´ Ù‡Ø§ÛŒ Ø¹Ù„ÙˆÙ… Ø§Ù‚ØªØµØ§Ø¯ÙŠØŸ\"\n",
        "print(f\"[Ø´Ù…Ø§]: {query_3}\")\n",
        "answer_3 = ask_robot(query_3)\n",
        "print(f\"\\n[Ø±Ø¨Ø§Øª]:\\n{answer_3}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "query_4 = \"Ø§Ø³Ù… Ù…Ù† Ú†ÛŒÙ‡ØŸ\"\n",
        "print(f\"[Ø´Ù…Ø§]: {query_4}\")\n",
        "answer_4 = ask_robot(query_4)\n",
        "print(f\"\\n[Ø±Ø¨Ø§Øª]:\\n{answer_4}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMzNn+PdmPwiENlA2XRQFzl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}